\section{Physical Essence of Language and Knowledge Graphs: Tensor Network States in Observer Networks}

In Section 22.3, we defined objective reality as Nash equilibrium of multi-agent games. This equilibrium state can be maintained because observers continuously engage in efficient information exchange (Bayesian updates). This raises a mechanism usually ignored by physics, but crucial when constructing "consensus geometry"—\textbf{Language}.

In QCA discrete ontology, language is by no means merely vibrations of air or accumulation of ink marks; it has profound physical essence. This section will prove that language is a set of \textbf{Quantum Communication Protocols} evolved by observers to establish and maintain \textbf{Long-range Entanglement}. And the knowledge system shared by all humans (or all intelligent agent networks)—\textbf{Knowledge Graph}—is mathematically isomorphic to \textbf{Tensor Network States} defined on joint Hilbert space of observers. Evolution of culture is the physical cooling process of this huge tensor network seeking ground state (minimizing communication free energy).

\subsection{Physical Operations of Symbols: Teleportation of Local Entanglement}

In Section 18.4, we defined "reference" as entanglement between internal symbol $S$ and external object $O$. Now consider two observers Alice and Bob, how do they confirm they are talking about the same "apple"?

\begin{definition}[Language Operator]
\label{def:language-operator}
Language is a set of unitary operators $\mathcal{L} = \{U_{word}\}$ acting on observer boundary algebra $\mathcal{A}_{\partial}$.

When Alice utters word "$W$", she executes an operation, mapping part of state in her internal model $\rho_A$ to public channel (sound waves/light waves):
$$|\rho_{channel} = \text{Tr}_{A} (U_{W} \rho_A U_{W}^\dagger)$$

Bob receives signal and executes inverse operation (understanding), updating his internal model $\rho_B$.
\end{definition}

\begin{theorem}[Entanglement Swapping of Semantics]
\label{thm:entanglement-swapping}
Successful communication (understanding) is physically equivalent to \textbf{Entanglement Swapping}.

\begin{enumerate}
\item Alice is entangled with "apple": $|\psi\rangle_{A-Apple}$.

\item Alice and Bob establish entanglement (or classical strong correlation) through language: $|\phi\rangle_{A-B}$.

\item Through joint measurement of language signals (understanding), Bob indirectly establishes correlation with "apple":
$$\rho_{B-Apple} \approx \text{Tr}_A (|\psi\rangle \otimes |\phi\rangle \dots)$$
\end{enumerate}

This means \textbf{language's function is to transform local private entanglement into non-local shared entanglement}. The "meaning" of a word is the \textbf{Fidelity} of entanglement channels it can establish in observer networks.
\end{theorem}

\subsection{Knowledge Graph as Tensor Network State (TNS)}

When thousands of observers connect through language, they collectively constitute a huge \textbf{Semantic Space}. In QCA theory, this space has strict mathematical structure.

\begin{definition}[Semantic Tensor Network]
\label{def:semantic-tensor-network}
Let observer network be graph $G=(V, E)$. Collective cognitive state $|\Psi_{Culture}\rangle$ of the entire system can be represented by a \textbf{Tensor Network}, such as \textbf{PEPS (Projected Entangled Pair States)} or \textbf{MERA (Multi-scale Entanglement Renormalization Ansatz)}.

\begin{itemize}
\item \textbf{Nodes (Tensors)}: Each observer (or concept node) corresponds to a tensor $T^{[i]}_{ \alpha \beta \gamma \dots}$. Tensor indices represent semantic connections with neighbors.

\item \textbf{Edges (Contraction)}: Language communication corresponds to contraction of tensor indices. Shared knowledge means two tensors are connected on specific "semantic indices."
\end{itemize}
\end{definition}

\textbf{Physical Picture}:

Human knowledge base (Wikipedia, scientific laws) is not bits stored on hard drives, but \textbf{Multi-body Entangled States} diffused in QCA subnetworks constituted by all human brains (and auxiliary devices).

Each piece of knowledge (e.g., "$E=mc^2$") is a \textbf{Strong Correlation Loop} in this tensor network. If you cut this loop (forget), overall entanglement entropy ($\Phi$) of the network will decrease.

\subsection{Semantic Geometry: Metric and Curvature of Concept Space}

Since knowledge graphs are tensor networks, they naturally possess geometric properties (this is also the foundation of holographic principle: Tensor Networks $\cong$ Geometry).

\begin{definition}[Semantic Distance]
\label{def:semantic-distance}
\textbf{Semantic Distance} $d(C_1, C_2)$ between two concepts $C_1$ and $C_2$ is defined as \textbf{Geodesic Length} (minimum number of tensor operation steps) connecting these two nodes in tensor network.

This explains why we feel "cat" and "dog" are closer than "cat" and "microwave oven"—because in semantic tensor networks, the former only needs to pass through few intermediate tensors (animal, pet) to connect, while the latter requires a long path.
\end{definition}

\begin{corollary}[Semantic Curvature and Polysemy]
\label{cor:semantic-curvature}
Geometry of semantic space is not always flat.

\begin{itemize}
\item \textbf{Flat Regions}: Logically rigorous scientific terms (such as mathematical definitions). Path integrals are path-independent, meaning is single-valued.

\item \textbf{High Curvature Regions}: Poetry, metaphors, or polysemy. From concept A to concept B, if taking different associative paths ($A \to \dots \to B$), different tensor phases are obtained (context-dependent).

\textbf{Ambiguity in language is manifestation of non-zero curvature (Berry Curvature) of semantic manifolds}.
\end{itemize}
\end{corollary}

\subsection{Cultural Evolution: Cooling Toward Ground State}

Language and knowledge are not static; they evolve over time. What physical laws does this evolution follow?

\begin{theorem}[Cultural Cooling Theorem]
\label{thm:cultural-cooling}
Dynamics of cultural evolution is equivalent to \textbf{Imaginary Time Evolution}, aiming to find \textbf{ground state} of "Semantic Hamiltonian."

Let $H_{sem}$ be Hamiltonian measuring communication misunderstanding (prediction error):
$$H_{sem} = \sum_{\langle i,j \rangle} J_{ij} (1 - \delta(w_i, w_j))$$

(If two people have different understanding of word $w$, energy increases).

Entire society continuously performs \textbf{Variational Optimization} on tensor network through dialogue, education, publication (QCA updates), to minimize $\langle \Psi | H_{sem} | \Psi \rangle$.

\begin{itemize}
\item \textbf{Dialect Unification}: This is symmetry breaking process (choosing a vacuum state).

\item \textbf{Scientific Progress}: This is process of finding new ground states at lower energy levels (lower prediction error).
\end{itemize}
\end{theorem}

\textbf{Conclusion}

Essence of language and knowledge graphs is \textbf{Physical}.

\begin{enumerate}
\item \textbf{Structure}: They are Tensor Network States (TNS) constructed on observer networks.

\item \textbf{Function}: They "stitch" private subjective models into huge objective consensus manifolds through entanglement swapping.

\item \textbf{Dynamics}: Cultural history is the cooling history of this tensor network continuously renormalizing and seeking lowest entropy states under information pressure.
\end{enumerate}

At this point, \textbf{Volume IV: Physics of Agency} is complete. Starting from algebraic definition of observers, through self-referential dynamics and consciousness topology, we finally reached social physics of multi-agent consensus. We proved: \textbf{Consciousness and language are not bystanders of physical universe, but the physical mechanisms of universe's self-cognition itself}.

In the final volume of the entire book—\textbf{Volume V: Metatheory — Logic, Computation, and Experimental Verification}—we will step out of specific physical models, examine mathematical structure of this theory from the height of category theory, and propose ultimate experimental verification schemes: Can we create artificial agents with topological consciousness in laboratories?

\textbf{(End of Volume IV)}

